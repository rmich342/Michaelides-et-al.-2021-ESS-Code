{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview:\n",
    "This Notebook goes through the following processing steps:\n",
    "1. Locate potential crossing locations between all beams\n",
    "2. Identifies valid crossovers from the list generated in step 1 according to user-specified criteria, and estimates elevation change (dh) at these crossing locations\n",
    "3. Temporally subsets results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5(fname, vnames=[]):\n",
    "    \"\"\" Simple HDF5 reader. \"\"\" #from IS2 hackweek tutorial: https://github.com/ICESAT-2HackWeek/2020_ICESat-2_Hackweek_Tutorials/blob/main/08.HDF5_and_ICESat-2_data_files/intro-is2-files_rendered.ipynb\n",
    "    import h5py\n",
    "    with h5py.File(fname, 'r') as f:\n",
    "        return [f[v][:] for v in vnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Locate Potential Crossing Locations\n",
    "(note: This function can take a long time (up to an hour or so) depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarse_xovers(data_dir, outfile):\n",
    "    \"\"\"\n",
    "    performs pairwise search for intersection between all tracks in the list. For every pair of tracks, this algorithm:\n",
    "\n",
    "    -Divides the tracks into 10-km segements (a lengthscale over which we assume the tracks can be approximated as linear)\n",
    "    -Fits a line to each the segmenets using linear regression\n",
    "    -Solves for the intersection between the two segements\n",
    "    -If the intersection point lies within the lat/lon bounds for the given segments, this is flagged as a valid crossover, and the files names, dates, and coordinates of the intersection are recorded and saves the results in an hdf5 file listing\n",
    "    the file name, rgt, dates, and intersection coordinates in UTM (zone 6) for each crossover are recorded\n",
    "    \n",
    "    The list of potential crossovers is saved as an hdf5 file. \n",
    "    \n",
    "    Parameters:\n",
    "    data_dir: directory with ATL06 files in reduced hdf5 format\n",
    "    outfile: name of output hdf5 file\n",
    "    \"\"\"\n",
    "    #import necessary packages\n",
    "    import time\n",
    "    import os\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    import pandas as pd\n",
    "    import glob as glob\n",
    "    from pyproj import Transformer\n",
    "    \n",
    "    start = time.time() #start timer\n",
    "    \n",
    "    #define some helper functions\n",
    "    def read_h5(fname, vnames=[]):\n",
    "        \"\"\" Simple HDF5 reader. \"\"\"\n",
    "        with h5py.File(fname, 'r') as f:\n",
    "            return [f[v][:] for v in vnames]\n",
    "        \n",
    "    def get_slope_intercept(x, y):\n",
    "        \"\"\"\n",
    "        performs a linear interpolation and returns the slope and intercept\n",
    "        \n",
    "        \"\"\"\n",
    "        m_a, c_a, r_a, p_a, sigma_a = stats.linregress(x,y)\n",
    "        return m_a, c_a\n",
    "    def get_rgt(fname):\n",
    "        return int(fname[31:35])\n",
    "    def get_beam(fname):\n",
    "        return(fname[47:51])\n",
    "    def get_date(fname):\n",
    "        return fname[16:24]\n",
    "    #gather files\n",
    "    os.chdir(data_dir)\n",
    "    files = glob.glob('*.h5')\n",
    "    #initialize lists\n",
    "    track_1_name = [] #file name for first track (string)\n",
    "    track_2_name = []\n",
    "    date_1 = [] #date of first track (int)\n",
    "    date_2 = []\n",
    "    intersection_x = [] #x-coordinate of estimated intersection (in UTM)\n",
    "    intersection_y = [] #x-coordinate of estimated intersection (in UTM)\n",
    "    #define latitudinal bands of 10 km to search (assuming tracks are ~linear on 10-km lengthscale)\n",
    "    lat_bands = np.linspace(7600000, 7730000, 14)\n",
    "    #loop through files\n",
    "    for i in range(0, len(files)):\n",
    "        track_name_a = files[i] #get filename\n",
    "        lon_a, lat_a, date_a =  read_h5(track_name_a, ['lon', 'lat', 't_year'])\n",
    "        #filter out points where the lon/lat is 0\n",
    "        lon_a = lon_a[(lon_a != 0) & (lat_a != 0)]\n",
    "        lat_a = lat_a[(lon_a != 0) & (lat_a != 0)]\n",
    "        #project into UTM Zone 6\n",
    "        transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32606\")\n",
    "        x_a, y_a = transformer.transform(lat_a, lon_a)\n",
    "        for j in range(i+1, len(files)): #get second track for comparison\n",
    "            if (j != i):\n",
    "                track_name_b = files[j] #get filename\n",
    "                lon_b, lat_b, date_b =  read_h5(track_name_b, ['lon', 'lat', 't_year'])\n",
    "                lon_b = lon_b[(lon_b != 0) & (lat_b != 0)]\n",
    "                lat_b = lat_b[(lon_b != 0) & (lat_b != 0)]\n",
    "                x_b, y_b = transformer.transform(lat_b, lon_b)\n",
    "                #if the two tracks are from the same rgt, check to see if they were direct repeats:\n",
    "                if((get_rgt(track_name_a) == get_rgt(track_name_b)) &(get_date(track_name_a) != get_date(track_name_b)) & \\\n",
    "                   (get_beam(track_name_a) == get_beam(track_name_b))):\n",
    "                    distance = []\n",
    "                    for a in range(0, len(lon_a)):\n",
    "                        #get distance between each point in track a and the closest point in track b\n",
    "                        x1 = x_a[a]\n",
    "                        y1 = y_a[a]\n",
    "                        distance.append(np.min(np.sqrt((x1 - x_b)**2 + (y1-y_b)**2))) \n",
    "                    if np.mean(distance) < 30: #if tracks are within 30m of each other, assume it's a repeat and move to next iteration (this may be a higher threshold than needed)\n",
    "                        continue\n",
    "                #loop through the latitudinal bands to find crossover\n",
    "                for l in range(0, (len(lat_bands) -1)):\n",
    "                    #define latitudinal band\n",
    "                    lat_min = lat_bands[l]\n",
    "                    lat_max = lat_bands[l+1]\n",
    "                    #subset track coordinates to latitudinal band:\n",
    "                    x_a_l = x_a[(y_a <= lat_max) & (y_a >= lat_min)]\n",
    "                    y_a_l = y_a[(y_a <= lat_max) & (y_a >= lat_min)]\n",
    "                    x_b_l = x_b[(y_b <= lat_max) & (y_b >= lat_min)]\n",
    "                    y_b_l = y_b[(y_b <= lat_max) & (y_b >= lat_min)]\n",
    "                    if ((len(x_a_l)<2) or (len(x_b_l) <2)):\n",
    "                        continue #move to next band if there's not enough data\n",
    "                    \n",
    "                    #perform linear fit on data to get the slope (m) and intercept(c)\n",
    "                    m_a, c_a = get_slope_intercept(x_a_l, y_a_l)\n",
    "                    m_b, c_b = get_slope_intercept(x_b_l, y_b_l)\n",
    "                    \n",
    "                    #find min/max longitude values in this band:\n",
    "                    lon_full = np.concatenate((x_a_l, x_b_l))\n",
    "                    lon_max = np.max(lon_full)\n",
    "                    lon_min = np.min(lon_full)\n",
    "                    lat_full = np.concatenate((y_a_l, y_b_l))\n",
    "                    lat_max = np.max(lat_full)\n",
    "                    lat_min = np.min(lat_full)\n",
    "\n",
    "                    if (m_a != m_b): #only solve for non-parallel lines\n",
    "                        #solve for crossing point between the two tracks\n",
    "                        x_i = (c_b - c_a)/(m_a - m_b)\n",
    "                        y_i = (c_a*m_b - c_b*m_a)/(m_b - m_a) \n",
    "                        #check if solution is within bounds of 10 km band\n",
    "                        if ((x_i < (lon_max)) & (x_i >(lon_min)) &(y_i < (lat_max)) & (y_i > (lat_min))):\n",
    "                            #order lists such that earlier date comes first\n",
    "                            if (date_a[0] > date_b[0]):\n",
    "                                #save data to lists\n",
    "                                track_1_name.append(track_name_b)\n",
    "                                track_2_name.append(track_name_a)\n",
    "                                date_1.append(date_b[0])\n",
    "                                date_2.append(date_a[0])\n",
    "                                intersection_x.append(x_i)\n",
    "                                intersection_y.append(y_i)\n",
    "                            elif(date_b[0] > date_a[0]):\n",
    "                                track_1_name.append(track_name_a)\n",
    "                                track_2_name.append(track_name_b)\n",
    "                                date_1.append(date_a[0])\n",
    "                                date_2.append(date_b[0])\n",
    "                                intersection_x.append(x_i)\n",
    "                                intersection_y.append(y_i)\n",
    "                            break #stop looping through bands if crossover found\n",
    "        #progress check at every 50th track:\n",
    "        if (i%50 ==0):\n",
    "            print('progress: files checked = ', i)\n",
    "    #convert data into dictionary, then dataframe\n",
    "    os.chdir('..')\n",
    "    data_dict = {'File_1': track_1_name, 'File_2': track_2_name, \\\n",
    "                'date_1': date_1, 'date_2': date_2, 'x': intersection_x, 'y': intersection_y}\n",
    "    data = pd.DataFrame(data_dict)\n",
    "    data.to_hdf(outfile, key = 'df') #save file\n",
    "    end = time.time()\n",
    "    print('time elapsed: ', (end - start)) #track time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coarse_xovers('Anaktuvuk_v2_ATL06_reduced_all_slopes', 'course_xovers_all_slopes_new_demo.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Identify valid crossovers and estimate elevation change (and other statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossovers(course_xovers, data_dir, radius, n_min, x_bounds, y_bounds, outfile, n_max = None, slope_filter = None):\n",
    "    \"\"\"\"\n",
    "    takes an h5 file with possible crossovers, generated using get_course_xovers, and calculates crossovers and relevant \n",
    "    statistics for all points that have at least n_min points within the specified radius of the crossing location. First, \n",
    "    the crossing location is re-estimated using a more localized linear interpolation based on the specified radius. The height \n",
    "    of each track at the crossing location is estimated using a linear interpolation of all points within the specified radius \n",
    "    from the crossing location, unless n_max is specified. The data can be further subsetted by a bounding box and along-track \n",
    "    slope threshold. The data is saved to the specified outfile in csv format.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    course_xovers(string): file with list of potential crossovers, generated via get_course_xovers\n",
    "    \n",
    "    data_dir(string): directory where ATL06 (in reduced h5 format) file are located\n",
    "    \n",
    "    radius: radius over which to search for data points and perform linear interpolation\n",
    "    \n",
    "    n_min (int): minimum number of points required within interpolation radius to be counted as a valid crossover\n",
    "    \n",
    "    x_bounds(array): x coordinates of bounding box to search over, as [lon_min, lon_max] in epsg 4326\n",
    "    \n",
    "    y_bounds(array): y coordinates of bounding box to search over, as [lat_min, lat_max] in epsg 4326\n",
    "    \n",
    "    outfile: name of output file (should be a csv)\n",
    "    \n",
    "    n_max (int): If specified, only use the n_max closest points when performing the linear interpolation\n",
    "    \n",
    "    slope_filter(double, m/m): If specified, ATL06 segments with an along-track slope higher than the filter will be removed \n",
    "    \n",
    "    \"\"\"\n",
    "    import h5py\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    from scipy import interpolate\n",
    "    import pandas as pd\n",
    "    from pyproj import Transformer\n",
    "    import time\n",
    "    import os\n",
    "    #define function for reading in hdf5 files\n",
    "    def read_h5(fname, vnames=[]):\n",
    "        \"\"\" Simple HDF5 reader. \"\"\"\n",
    "        with h5py.File(fname, 'r') as f:\n",
    "            return [f[v][:] for v in vnames]\n",
    "    #define functions for pulling relative attributes out of file names\n",
    "    def get_rgt(fname):\n",
    "        return int(fname[31:35])\n",
    "    def get_cycle(fname):\n",
    "        return int(fname[36])\n",
    "    def get_date(fname):\n",
    "        return fname[16:24]\n",
    "    def get_beam(fname):\n",
    "        return(fname[47:51])\n",
    "        \n",
    "    \n",
    "    \n",
    "    def intersection(x1, y1, x2, y2):\n",
    "        \"\"\"\n",
    "        uses a linear interpolation to estimate the intersection of two data sets\n",
    "        \"\"\"\n",
    "        #perform linear fit to each dataset\n",
    "        m_a, c_a, r_a, p_a, sigma_a = stats.linregress(x1,y1)\n",
    "        m_b, c_b, r_b, p_b, sigma_b = stats.linregress(x2,y2)\n",
    "        x_full = np.concatenate((x1, x2))\n",
    "        y_full = np.concatenate((y1, y2))\n",
    "        x_max = np.max(x_full)\n",
    "        x_min = np.min(x_full)\n",
    "        y_max = np.max(y_full)\n",
    "        y_min = np.min(y_full)\n",
    "        if (m_a != m_b): #only solve for non-parallel lines using slope (m) and intercept (c)\n",
    "            lon_i = (c_b - c_a)/(m_a - m_b)\n",
    "            lat_i = (c_a*m_b - c_b*m_a)/(m_b - m_a)\n",
    "            if ((lon_i < (x_max)) & (lon_i >(x_min)) &(lat_i < (y_max)) & (lat_i > (y_min))): #make sure solution is in lat/lon range of the two tracks\n",
    "                return lon_i, lat_i\n",
    "            else:\n",
    "                lon_i = float(\"nan\")\n",
    "                lat_i = float(\"nan\")\n",
    "                return lon_i, lat_i\n",
    "        else:\n",
    "            lon_i = float(\"nan\")\n",
    "            lat_i = float(\"nan\")\n",
    "            return lon_i, lat_i\n",
    "    start = time.time() #start timer\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32606\") #transform from ESPG 4326 (lat/lon) to UTM zone 6\n",
    "    transformer_i = Transformer.from_crs(\"epsg:32606\", \"epsg:4326\") #transform from UTM zone 6 to ESPG 4326 (lat/lon)\n",
    "    \n",
    "    #initialize lists for storage\n",
    "    #crossover coordinates in UTM\n",
    "    crossover_x = [] \n",
    "    crossover_y = []\n",
    "    #estimated height at crossing point for each track (m)\n",
    "    height_1 = [] \n",
    "    height_2 = [] \n",
    "    dh_elev = [] #estimated elevation change at crossing point (m)\n",
    "    #propogated uncertainty of crossing height on each track (m)\n",
    "    sigma_1 = [] \n",
    "    sigma_2 = [] \n",
    "    sigma_c = [] #propogated uncertainty of elevation change (m)\n",
    "    #time/date of each track\n",
    "    time_1 = [] \n",
    "    time_2 = []\n",
    "    delta_time = [] #time interval between the two tracks\n",
    "    #file name of each track\n",
    "    f1 = [] \n",
    "    f2 = [] \n",
    "    #number of points used for interpolation for each track\n",
    "    n1 = [] \n",
    "    n2 = []\n",
    "    #mean number of photons per segement for each track\n",
    "    n_photons_1 = []\n",
    "    n_photons_2 = []\n",
    "    #mean along-track slope for each track \n",
    "    dh_dx_1 = []\n",
    "    dh_dx_2 = []\n",
    "    #mean across-track slope for each track\n",
    "    dh_dy_1 = []\n",
    "    dh_dy_2 = []\n",
    "    #sum of the square of the residuals from the linear fit for each track\n",
    "    resid_1 = []\n",
    "    resid_2 = []\n",
    "    \n",
    "    #read in crossover list\n",
    "    course_crossovers = pd.read_hdf(course_xovers)\n",
    "    os.chdir(data_dir)\n",
    "    for f in range(0, len(course_crossovers)):\n",
    "        #read in files and reproject to UTM\n",
    "        file_1 = (course_crossovers['File_1']).values[f]\n",
    "        lon1, lat1, t1, h1, dh_dx1, dh_dy1, sigma_h1, n_ph1 = read_h5(file_1, ['lon', 'lat', 't_year', 'h_elv', 'dh_dx', \\\n",
    "                                                                              'dh_dy', 's_elv', 'n_photons'])\n",
    "        \n",
    "        #filter out points where lon/lat = 0\n",
    "        lon1 = lon1[(lon1 !=0) & (lat1 !=0)]\n",
    "        lat1 = lat1[(lon1 !=0) & (lat1 !=0)]\n",
    "        #only keep points in spatial bounds\n",
    "        x_min = x_bounds[0]\n",
    "        x_max = x_bounds[1]\n",
    "        y_min = y_bounds[0]\n",
    "        y_max = y_bounds[1]\n",
    "        index_1 = ((lon1 > x_min)&(lon1 < x_max) & (lat1 > y_min) & (lat1 < y_max))\n",
    "        lon1 = lon1[index_1]\n",
    "        if(len(lon1)) ==0:\n",
    "            continue\n",
    "        lat1 = lat1[index_1]\n",
    "        t1 = t1[index_1]\n",
    "        h1 = h1[index_1]\n",
    "        dh_dx1 = dh_dx1[index_1]\n",
    "        dh_dy1 = dh_dy1[index_1]\n",
    "        sigma_h1 = sigma_h1[index_1]\n",
    "        n_ph1 = n_ph1[index_1]\n",
    "        #reproject into UTM zone 6\n",
    "        lon1_p, lat1_p = transformer.transform(lat1, lon1)\n",
    "        file_2 = (course_crossovers['File_2']).values[f]\n",
    "        if file_1 == file_2: #skip duplicates\n",
    "            continue\n",
    "        #read in 2nd file\n",
    "        lon2, lat2, t2, h2, dh_dx2, dh_dy2, sigma_h2, n_ph2 = read_h5(file_2, ['lon', 'lat', 't_year', 'h_elv', 'dh_dx', \\\n",
    "                                                                               'dh_dy', 's_elv', 'n_photons'])\n",
    "        lon2 = lon2[(lon2 !=0) & (lat2 !=0)]\n",
    "        lat2 = lat2[(lon2 !=0) & (lat2 !=0)]\n",
    "        index_2 = ((lon2 > x_min)&(lon2 < x_max) & (lat2 > y_min) & (lat2 < y_max))\n",
    "        lon2 = lon2[index_2]\n",
    "        if(len(lon2)) ==0:\n",
    "            continue\n",
    "        lat2 = lat2[index_2]\n",
    "        t2 = t2[index_2]\n",
    "        h2 = h2[index_2]\n",
    "        dh_dx2 = dh_dx2[index_2]\n",
    "        dh_dy2 = dh_dy2[index_2]\n",
    "        sigma_h2 = sigma_h2[index_2]\n",
    "        n_ph2 = n_ph2[index_2]\n",
    "        lon2_p, lat2_p = transformer.transform(lat2, lon2)\n",
    "        #check for potential repeats (they should have all been removed in the course crossovers check, but this is to make sure)\n",
    "        if((get_rgt(file_1) == get_rgt(file_2)) &(get_date(file_1) != get_date(file_2)) & \\\n",
    "                   (get_beam(file_1) == get_beam(file_2))):\n",
    "                    distance = []\n",
    "                    for i in range(0, len(lon1_p)):\n",
    "                        #get distance between points on track one and closest point on track 2\n",
    "                        x1 = lon1_p[i]\n",
    "                        y1 = lat1_p[i]\n",
    "                        distance.append(np.min(np.sqrt((x1 - lon2_p)**2 + (y1-lat2_p)**2)))\n",
    "                    if np.mean(distance) < 30: #if tracks are within 30m of each other, assume it's a repeat and move to next iteration (this may be a higher threshold than needed)\n",
    "                        continue\n",
    "        \n",
    "        #read in course crossover estimate\n",
    "        lon_c = (course_crossovers['x']).values[f]\n",
    "        lat_c = (course_crossovers['y']).values[f]\n",
    "        #get distance from crossover location for points in each line\n",
    "        distance_1 = np.sqrt((lon1_p - lon_c)**2 + (lat1_p - lat_c)**2)\n",
    "        distance_2 = np.sqrt((lon2_p - lon_c)**2 + (lat2_p - lat_c)**2)\n",
    "        if(radius >= 2000): #set upper bounds on initial search radius\n",
    "            r2 = 10000\n",
    "        else:\n",
    "            r2 = 5*radius #initial search radius - start with larger radius in case our original crossover location estimate was inaccurate\n",
    "        #filter for points within r2 for intermediate crossover calculation \n",
    "        lon1_f_i = lon1_p[abs(distance_1) < r2] \n",
    "        lat1_f_i = lat1_p[abs(distance_1) < r2]\n",
    "        lon2_f_i = lon2_p[abs(distance_2) < r2]\n",
    "        lat2_f_i = lat2_p[abs(distance_2) < r2]\n",
    "        #move to next pair if there aren't any points in either line within the larger search radius\n",
    "        if((len(lon1_f_i) < 2) or (len(lon2_f_i) < 2)):\n",
    "            continue\n",
    "        #re-calculate crossover location\n",
    "        x_i, y_i = intersection(lon1_f_i, lat1_f_i, lon2_f_i, lat2_f_i)\n",
    "        #make sure new crossover location is valid\n",
    "        if(np.isnan(x_i)):\n",
    "            continue\n",
    "        #re-calculate distance from new crossover location\n",
    "        distance_1_i = np.sqrt((lon1_p - x_i)**2 + (lat1_p - y_i)**2)\n",
    "        distance_2_i = np.sqrt((lon2_p - x_i)**2 + (lat2_p - y_i)**2)\n",
    "        #keep data for points within final search radius \n",
    "        lon1_f = lon1_p[distance_1_i < radius]\n",
    "        lat1_f = lat1_p[distance_1_i < radius]\n",
    "        h1_f = h1[distance_1_i < radius]\n",
    "        sigma_h1_f = sigma_h1[distance_1_i < radius]\n",
    "        t1_f = t1[distance_1_i < radius]\n",
    "        dh_dx1_f = dh_dx1[distance_1_i < radius]\n",
    "        dh_dy1_f = dh_dy1[distance_1_i < radius]\n",
    "        n_ph1_f = n_ph1[distance_1_i < radius]\n",
    "        \n",
    "        distance_2 = np.sqrt((lon2_p - lon_c)**2 + (lat2_p - lat_c)**2)\n",
    "        lon2_f = lon2_p[distance_2_i < radius]\n",
    "        lat2_f = lat2_p[distance_2_i < radius]\n",
    "        h2_f = h2[distance_2_i < radius]\n",
    "        t2_f = t2[distance_2_i < radius]\n",
    "        dh_dx2_f = dh_dx2[distance_2_i < radius]\n",
    "        dh_dy2_f = dh_dy2[distance_2_i < radius]\n",
    "        sigma_h2_f = sigma_h2[distance_2_i < radius]\n",
    "        n_ph2_f = n_ph2[distance_2_i < radius]\n",
    "        \n",
    "        #make sure there are enough points before continuing\n",
    "        if n_max != None:\n",
    "            #sort points by distance from crossover and extract the n_max closest points\n",
    "            sort_index1 = np.argsort(distance_1)\n",
    "            indicies_1 = sort_index1[0:n_max]\n",
    "            sort_index2 = np.argsort(distance_2)\n",
    "            indicies_2 = sort_index2[0:n_max]\n",
    "            lat1_f = lat1_f[indices_1]\n",
    "            lon1_f = lon1_f[indices_1]\n",
    "            h1_f = h1_f[indices_1]\n",
    "            t1_f = t1_f[indices_1]\n",
    "            sigma_h1_f = sigma_h1_f[indices_1]\n",
    "            lat2_f = lat2_f[indices_2]\n",
    "            lon2_f = lon2_f[indices_2]\n",
    "            h2_f = h2_f[indices_2]\n",
    "            t2_f = t2_f[indices_2]\n",
    "            sigma_h2_f = sigma_h2_f[indices_2]\n",
    "        \n",
    "        if slope_filter != None: #filter by along-track slope if specified\n",
    "            indices_1 = abs(dh_dx1_f) < slope_filter\n",
    "            lon1_f = lon1_f[indices_1]\n",
    "            lat1_f = lat1_f[indices_1]\n",
    "            h1_f = h1_f[indices_1]\n",
    "            sigma_h1_f = sigma_h1_f[indices_1]\n",
    "            t1_f = t1_f[indices_1]\n",
    "            dh_dx1_f = dh_dx1_f[indices_1]\n",
    "            dh_dy1_f = dh_dy1_f[indices_1]\n",
    "            n_ph1_f = n_ph1_f[indices_1]\n",
    "            \n",
    "            indices_2 = abs(dh_dx2_f) < slope_filter\n",
    "            lon2_f = lon2_f[indices_2]\n",
    "            lat2_f = lat2_f[indices_2]\n",
    "            h2_f = h2_f[indices_2]\n",
    "            sigma_h2_f = sigma_h2_f[indices_2]\n",
    "            t2_f = t2_f[indices_2]\n",
    "            dh_dx2_f = dh_dx2_f[indices_2]\n",
    "            dh_dy2_f = dh_dy2_f[indices_2]\n",
    "            n_ph2_f = n_ph2_f[indices_2]\n",
    "        \n",
    "        if (len(lon1_f) < n_min or (len(lon2_f) < n_min)): #make sure there are enough points post-filtering\n",
    "            continue\n",
    "            \n",
    "        #re-interpolate with subsetted points to get final crossing location\n",
    "        x_c_f, y_c_f = intersection(lon1_f, lat1_f, lon2_f, lat2_f)\n",
    "        \n",
    "        if(np.isnan(x_c_f)):\n",
    "            continue\n",
    "        #get along-track distance\n",
    "        distance_1_f = np.sqrt((lon1_f - lon1_f[0])**2 + (lat1_f - lat1_f[0])**2)\n",
    "        distance_2_f = np.sqrt((lon2_f - lon2_f[0])**2 + (lat2_f - lat2_f[0])**2)\n",
    "\n",
    "        #get weights from uncertainties\n",
    "        weights_1 = sigma_h1_f**(-1)\n",
    "        #get linear fit of height as a function of along-track distance, weighted by the uncertainties on each height measurement\n",
    "        p1, resid, rank, singular, rcond = np.polyfit(distance_1_f, h1_f, 1, full = True, w = weights_1, cov = 'unscaled')\n",
    "        res_1 = np.polyval(p1, distance_1_f) - h1_f #residuals\n",
    "        residuals_1 = (np.sum(res_1**2))#sum of the square of the residuals\n",
    "        #initialize uncertainties\n",
    "        sigma_m_1 = float('nan') #uncertainty on the slope\n",
    "        sigma_c_1 = float('nan') #uncertainty on the intercept\n",
    "        sigma_r_1 = float('nan') #uncertainty contribution of the residuals\n",
    "        \n",
    "        \n",
    "        if(len(distance_1_f)) == 2: \n",
    "            #manually solve for slope/intercept uncertainties for 2 points\n",
    "            sigma_m_1 = np.sqrt(sigma_h1_f[0]**2 + sigma_h1_f[1]**2)/(distance_1_f[1])\n",
    "            sigma_r_1 = 0\n",
    "            sigma_c_1 = sigma_h1_f[0]\n",
    "\n",
    "        else:\n",
    "            #get the slope/intercept uncertainties from the covariance matrix\n",
    "            p1, v1 = np.polyfit(distance_1_f, h1_f, 1, w = weights_1, cov = 'unscaled')\n",
    "            sigma_m_1 = np.sqrt(v1[0][0])\n",
    "            sigma_c_1 = np.sqrt(v1[1][1])\n",
    "            sigma_r_1 = np.sqrt(residuals_1/(len(distance_1_f) - 2))\n",
    "\n",
    "\n",
    "        #get along-track distance of crossing point\n",
    "        distance_c1 = np.sqrt((x_c_f - lon1_f[0])**2 + (y_c_f - lat1_f[0])**2)\n",
    "        #interpolate height to crossing point\n",
    "        h1_interp = np.polyval(p1, distance_c1) #fit at crossing location\n",
    "        #fit and interpolate other variables:\n",
    "        #time\n",
    "        p1_t = np.polyfit(distance_1_f, t1_f, 1)\n",
    "        t1_interp = np.polyval(p1_t, distance_c1)\n",
    "        #along-track slope\n",
    "        p1_dh_dx = np.polyfit(distance_1_f, dh_dx1_f, 1)\n",
    "        dh_dx_i_1 = np.polyval(p1_dh_dx,distance_c1 )\n",
    "        #across track slope\n",
    "        p1_dh_dy = np.polyfit(distance_1_f, dh_dy1_f, 1)\n",
    "        dh_dy_i_1 = np.polyval(p1_dh_dx, distance_c1)\n",
    "        \n",
    "        \n",
    "        #repeat for second track\n",
    "        weights_2 = sigma_h2_f**(-1)\n",
    "        #get linear fit and the sum of the residual\n",
    "        p2, resid, rank, singular, rcond = np.polyfit(distance_2_f, h2_f, 1, full = True, w = weights_2, cov = 'unscaled')\n",
    "        res_2 = p2[0]*distance_2_f + p2[1] - h2_f #residuals\n",
    "        residuals_2 = (np.sum(res_2**2))#sum of the square of the residuals\n",
    "\n",
    "        if(len(distance_2_f)) == 2: \n",
    "            #manually solve for slope/intercept uncertainties for 2 points\n",
    "            sigma_m_2 = np.sqrt(sigma_h2_f[0]**2 + sigma_h2_f[1]**2)/(distance_2_f[1])\n",
    "            sigma_r_2 = 0\n",
    "            sigma_c_2 = sigma_h2_f[0]\n",
    "        else:\n",
    "            #get the slope/intercept uncertainties from the covariance matrix\n",
    "            p2, v2 = np.polyfit(distance_2_f, h2_f, 1, w = weights_2, cov = 'unscaled')\n",
    "            sigma_m_2 = np.sqrt(v2[0][0])\n",
    "            sigma_c_2 = np.sqrt(v2[1][1])\n",
    "            sigma_r_2 = np.sqrt(residuals_2/(len(distance_2_f)-2))\n",
    "        #get along-track distance of crossing\n",
    "        distance_c2 = np.sqrt((x_c_f - lon2_f[0])**2 + (y_c_f - lat2_f[0])**2)\n",
    "        h2_interp = np.polyval(p2, distance_c2)\n",
    "        #interpolate other variables\n",
    "        p2_t = np.polyfit(distance_2_f, t2_f, 1)\n",
    "        t2_interp = np.polyval(p2_t, distance_c2)\n",
    "        p2_dh_dx = np.polyfit(distance_2_f, dh_dx2_f, 1)\n",
    "        dh_dx_i_2 = np.polyval(p2_dh_dx,distance_c2)\n",
    "        p2_dh_dy  = np.polyfit(distance_2_f, dh_dy2_f, 1)\n",
    "        dh_dy_i_2 = np.polyval(p2_dh_dy, distance_c2 )\n",
    "        \n",
    "        #get dh and dt at crossover location\n",
    "        dh = h2_interp - h1_interp\n",
    "        dt = t2_interp - t1_interp\n",
    "        \n",
    "        #calculate uncertainties on interpolated heights\n",
    "        var_h1 = ((distance_c1 - np.mean(distance_1_f))**2)*sigma_m_1**2 +  sigma_r_1**2#variance of h1_interp\n",
    "        var_h2 = ((distance_c2 - np.mean(distance_2_f))**2)*sigma_m_2**2 +  sigma_r_2**2\n",
    "        sigma_dh = np.sqrt(var_h1 + var_h2) #uncertainty on crossover height difference\n",
    "\n",
    "        #aggregate data\n",
    "        crossover_x.append(x_c_f)\n",
    "        crossover_y.append(y_c_f)\n",
    "        height_1.append(h1_interp)\n",
    "        height_2.append(h2_interp)\n",
    "        dh_elev.append(dh)\n",
    "        time_1.append(t1_interp)\n",
    "        time_2.append(t2_interp)\n",
    "        delta_time.append(dt)\n",
    "        n1.append(len(lon1_f))\n",
    "        n2.append(len(lon2_f))\n",
    "        f1.append(file_1)\n",
    "        f2.append(file_2)\n",
    "        dh_dx_1.append(dh_dx_i_1)\n",
    "        dh_dx_2.append(dh_dx_i_2)\n",
    "        dh_dy_1.append(dh_dy_i_1)\n",
    "        dh_dy_2.append(dh_dy_i_2)\n",
    "        sigma_1.append(np.sqrt(var_h1))\n",
    "        sigma_2.append(np.sqrt(var_h2))\n",
    "        sigma_c.append(sigma_dh)\n",
    "        n_photons_1.append(np.mean(n_ph1_f))\n",
    "        n_photons_2.append(np.mean(n_ph2_f))\n",
    "        resid_1.append(residuals_1)\n",
    "        resid_2.append(residuals_2)\n",
    "\n",
    "        \n",
    "       #save data into dataframe \n",
    "    os.chdir('..')\n",
    "    dt_days = [d* 365 for d in delta_time] #convert time interval from years to days\n",
    "    crossover_lat, crossover_lon = transformer_i.transform(crossover_x, crossover_y) #project back into lon/lat\n",
    "    #store data as dictionary\n",
    "    data_dict = {'x': crossover_x, 'y': crossover_y, 'lon': crossover_lon, 'lat': crossover_lat, 'dh': dh_elev,\\\n",
    "                 'sigma_dh': sigma_c, 'h1_interp': height_1, 'h2_interp': height_2,'time_1': time_1, 'time_2': time_2, \\\n",
    "                 'dt(decimal year)': delta_time, 'dt (days)':dt_days, 'source_1': f1, 'source_2': f2, 'n_points(track_1)': n1,\\\n",
    "                 'n_points(track_2)': n2,'sigma_h1':sigma_1, 'sigma_h2': sigma_2, 'dh_dx_1': dh_dx_1, 'dh_dy_1': dh_dy_1, \\\n",
    "                'dh_dx_2': dh_dx_2, 'dh_dy_2': dh_dy_2, 'track_1_fit': resid_1, 'track_2_fit': resid_2, \\\n",
    "                 'n_photons_1': n_photons_1, 'n_photons_2': n_photons_2}\n",
    "    \n",
    "    \n",
    "    data = pd.DataFrame(data_dict) #convert dictionary to dataframe\n",
    "    data.to_csv(outfile) #save as csv\n",
    "    end = time.time()\n",
    "    print('time_elapsed: ', (end - start)) #record total time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell reproduces the datasets used in Michaelides et al (2021). We tested this algorithm with interpolation radii varying from 20 to 100 m, with n_min set such that the along-track point desnity was at one segment every 40 m on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossovers('course_xovers_all_slopes_new_demo.h5','Anaktuvuk_v2_ATL06_reduced_all_slopes', 20, 2,[-151.4259, -148.56], [68.5275, 69.5649],'xovers_full_period_low_slopes_new_demo.csv',slope_filter = 0.05)\n",
    "#crossovers('course_xovers_all_slopes_new_demo.h5','Anaktuvuk_v2_ATL06_reduced_all_slopes', 40, 2,[-151.4259, -148.56], [68.5275, 69.5649],'xovers_40_full_period_low_slopes_new_demo.csv',slope_filter = 0.05)\n",
    "#crossovers('course_xovers_all_slopes_new_demo.h5','Anaktuvuk_v2_ATL06_reduced_all_slopes', 60, 3,[-151.4259, -148.56], [68.5275, 69.5649],'xovers_60_full_period_low_slopes_new_demo.csv',slope_filter = 0.05)\n",
    "#crossovers('course_xovers_all_slopes_new_demo.h5','Anaktuvuk_v2_ATL06_reduced_all_slopes', 80, 4,[-151.4259, -148.56], [68.5275, 69.5649],'xovers_80_full_period_low_slopes_new_demo.csv',slope_filter = 0.05)\n",
    "#crossovers('course_xovers_all_slopes_new_demo.h5','Anaktuvuk_v2_ATL06_reduced_all_slopes', 100, 5,[-151.4259, -148.56], [68.5275, 69.5649],'xovers_100_full_period_low_slopes_new_demo.csv',slope_filter = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Temporal Subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out crossovers from individual thaw seasons (defined here as March 8th-October 22 in 2019, equivlanet DOY in 2020) \n",
    "xovers_full_new = pd.read_csv('xovers_full_period_low_slopes_new_demo.csv')\n",
    "xovers_seasonal = xovers_full_new[((xovers_full_new['time_1'] - np.floor(xovers_full_new['time_1']))*365 >= 67) & \\\n",
    "                              ((xovers_full_new['time_2'] - np.floor(xovers_full_new['time_2']))*365 <= 295) & \\\n",
    "                              (np.floor(xovers_full_new['time_1']) > 2018)  &  (np.floor(xovers_full_new['time_2']) > 2018) & \\\n",
    "                             (np.floor(xovers_full_new['time_1']) == np.floor(xovers_full_new['time_2']))]\n",
    "xovers_seasonal.reset_index(inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xovers_seasonal.to_csv('xovers_two_seasons_low_slope_new_demo.csv') #save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
