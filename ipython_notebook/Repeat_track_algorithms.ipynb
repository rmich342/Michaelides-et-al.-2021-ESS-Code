{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook goes through locating repeat tracks and estimating the elevation change between points on repeated tracks with a distance of < 5m. We specifically focus on tracks from the 2019 thaw season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locating potential repeat tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repeat_tracks(data_dir):\n",
    "    \"Searches through a list of ATL06 files in the specified drectorry (data_dir) and finds potential repeats, defined as two beam of the same RGT and spot that are \\\n",
    "    within acorss-track distances of 45 m or less. The resulting list is returned as a pndas dataframe\"\n",
    "    #import packages\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import time\n",
    "    #define helper functions for pulling information from the file names\n",
    "    def get_rgt(fname):\n",
    "        return int(fname[31:35])\n",
    "    def get_cycle(fname):\n",
    "        return int(fname[36])\n",
    "    def get_date(fname):\n",
    "        return fname[16:24]\n",
    "    def get_beam(fname):\n",
    "        return(fname[47:51])\n",
    "    def read_h5(fname, vnames=[]):\n",
    "        \"\"\" Simple HDF5 reader. \"\"\"\n",
    "        with h5py.File(fname, 'r') as f:\n",
    "            return [f[v][:] for v in vnames]\n",
    "    \n",
    "    start = time.time() #start timer\n",
    "    os.chdir(data_dir)\n",
    "    files = glob.glob('*.h5') #get list of all ATL06 files\n",
    "    #intialize lists to store variables\n",
    "    #file names\n",
    "    file1 = []\n",
    "    file2 = []\n",
    "    rgt = [] #RGT number\n",
    "    #dates of repeat passes in YYYMMDD format\n",
    "    date_1 = []\n",
    "    date_2 = []\n",
    "    #dates of repeat passes in year/decimal format\n",
    "    date_1_decimal = []\n",
    "    date_2_decimal = []\n",
    "    \n",
    "    #loop through files\n",
    "    for i in range(1, len(files)):\n",
    "        #read in files and coordinates\n",
    "        file_1 = files[i]\n",
    "        x_atc1, y_atc1,t1 = read_h5(file_1,['x_atc', 'y_atc', 't_year']) #get along and across-track coordiantes\n",
    "        for j in range(i+1, len(files)):\n",
    "            file_2 = files[j]\n",
    "            x_atc2, y_atc2, t2 = read_h5(file_2,['x_atc', 'y_atc', 't_year'])\n",
    "            \n",
    "            #select tracks of the same rgt and beam but different dates\n",
    "            if((get_rgt(file_1) == get_rgt(file_2)) &(get_date(file_1) != get_date(file_2)) & (get_beam(file_1) == get_beam(file_2))):\n",
    "                x_atc = list(set(x_atc1) & set(x_atc2)) #get list of along-track coordinates common to both data sets\n",
    "                y1 = []\n",
    "                for x in range(0, len(x_atc1)):\n",
    "                    if(x_atc1[x] in x_atc): #save points in the common list of rgts\n",
    "                        y1.append(y_atc1[x])\n",
    "                y2 = []\n",
    "                for x in range(0, len(x_atc2)):\n",
    "                    if(x_atc2[x] in x_atc):\n",
    "                        y2.append(y_atc2[x])\n",
    "                if ((len(y1)==0) or (len(y2) ==0)):\n",
    "                    continue\n",
    "                if (np.max(abs(np.array(y2) - np.array(y1))) < 45): #only keep beams with across-track distance < 45 m (could be modified to be stricter)\n",
    "                    #save file name, date, and rgt\n",
    "                    file1.append(file_1)\n",
    "                    file2.append(file_2)\n",
    "                    rgt.append(get_rgt(file_1))\n",
    "                    date_1.append(get_date(file_1))\n",
    "                    date_1_decimal.append(t1[0])\n",
    "                    date_2.append(get_date(file_2))\n",
    "                    date_2_decimal.append(t2[0])\n",
    "                    distance = np.sqrt((np.array(y1) - np.array(y2))**2)\n",
    "        if (i%50 ==0): #update progress every 50 files\n",
    "            print('progress: ', i ,'files checked')\n",
    "    #convert output to dataframe\n",
    "    data_dict = {'file_1':file1, 'file_2': file2, 'rgt':rgt, 'date_1': date_1, 'date_1_decimal': date_1_decimal, \\\n",
    "                 'date_2_decimal': date_2_decimal, 'date_2': date_2}\n",
    "    data = pd.DataFrame(data_dict)\n",
    "    end = time.time()\n",
    "    print('time elapsed: ', (end - start))\n",
    "    os.chdir('..')\n",
    "    return data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:  50 files checked\n",
      "progress:  100 files checked\n",
      "progress:  150 files checked\n",
      "progress:  200 files checked\n",
      "progress:  250 files checked\n",
      "progress:  300 files checked\n",
      "progress:  350 files checked\n",
      "progress:  400 files checked\n",
      "progress:  450 files checked\n",
      "progress:  500 files checked\n",
      "progress:  550 files checked\n",
      "progress:  600 files checked\n",
      "progress:  650 files checked\n",
      "progress:  700 files checked\n",
      "progress:  750 files checked\n",
      "progress:  800 files checked\n",
      "progress:  850 files checked\n",
      "progress:  900 files checked\n",
      "progress:  950 files checked\n",
      "progress:  1000 files checked\n",
      "time elapsed:  959.6594512462616\n"
     ]
    }
   ],
   "source": [
    "#find all potetial repeats from our ATL06 data\n",
    "repeat_list = find_repeat_tracks('Anaktuvuk_v2_ATL06_reduced_all_slopes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_list.to_csv('repeat_track_list.csv', index = False) #save list for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset to 2019 thaw season - defined here as March 8th to October 22nd\n",
    "repeats_summer_19 = repeat_list_v3[(repeat_list_v3['date_1_decimal'] > 2019 + 2/12 + 8/365) & \\\n",
    "                             (repeat_list_v3['date_2_decimal'] < 2019 + 9/12 +22/365)]\n",
    "repeats_summer_19.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats_summer_19.to_csv('repeat_track_list_s_2019.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate elevation change at valid repeat points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_diffv2(repeat_tracks, data_dir):\n",
    "    \"Searches through a given list of potential repeat tracks for points that < 5m apart, and estaimtes the height difference. \\\n",
    "    The resulting height differences and relevant metrics are recorded and returned in a pandas dataframe\"\n",
    "    \n",
    "    #import needed packages\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import geopy.distance\n",
    "    from pyproj import Transformer\n",
    "    \n",
    "    #define some helper functions\n",
    "    def read_h5(fname, vnames=[]):\n",
    "        \"\"\" Simple HDF5 reader. \"\"\"\n",
    "        with h5py.File(fname, 'r') as f:\n",
    "            return [f[v][:] for v in vnames] \n",
    "\n",
    "    def get_rgt(fname):\n",
    "        return int(fname[31:35])\n",
    "\n",
    "    def get_spot(fname):\n",
    "        return(fname[47:51])\n",
    "    \n",
    "    start = time.time() #start timer\n",
    "    os.chdir(data_dir)\n",
    "    #intilize list for storing variables\n",
    "    #elevation change\n",
    "    dh = [] #elevation change (m)\n",
    "    sigma_dh = [] #uncertainty in elevation change (m)\n",
    "    #time of first and second pass\n",
    "    time_1 = []\n",
    "    time_2 = []\n",
    "    dt = [] #time interval between repeats (years)\n",
    "    #elevation on each date\n",
    "    height_1 = []\n",
    "    height_2 = []\n",
    "    #uncertainties on each height\n",
    "    s_h1 = []\n",
    "    s_h2 = []\n",
    "    #across-track position of each point\n",
    "    y_1 = []\n",
    "    y_2 = []\n",
    "    #coordinates of each point\n",
    "    lon_1 = []\n",
    "    lat_1 = []\n",
    "    lon_2 = []\n",
    "    lat_2 = []\n",
    "    #along and across-track slopes at each point\n",
    "    slope_across_1 = []\n",
    "    slope_across_2 = []\n",
    "    slope_along_1 = []\n",
    "    slope_along_2 = []\n",
    "    #source files\n",
    "    source_1 = []\n",
    "    source_2 = []\n",
    "    #along and across-track distances between each repeat point\n",
    "    distance_x = []\n",
    "    distance_y = []\n",
    "    #ATL06 fitting window (m)\n",
    "    window1 = []\n",
    "    window2 = []\n",
    "    \n",
    "    #for conversion from lat/lon to UTM zone 6\n",
    "    transformer = Transformer.from_crs(\"epsg:4326\", \"epsg:32606\") \n",
    "    for i in range(0, len(repeat_tracks)): #loop through repeat track list\n",
    "        #read in potential repeat files\n",
    "        file_1 = repeat_tracks['file_1'][i]\n",
    "        file_2 = repeat_tracks['file_2'][i]\n",
    "        #for r=each track, read in: coordinates, height, height uncertainty, time, along and across-trac slope, azimuth,\n",
    "        #and ATL06 fitting window\n",
    "        lon1, lat1, h1, sh_1, t1, dh_dx1, dh_dy1,az1, window_1 = read_h5(file_1, ['lon', 'lat', 'h_elv', \\\n",
    "                                                                                     's_elv','t_year', 'dh_dx', 'dh_dy', \\\n",
    "                                                                                  'azimuth', 'window_width'])\n",
    "        \n",
    "        lon2, lat2, h2, sh_2, t2, dh_dx2, dh_dy2,az2, window_2 = read_h5(file_2, ['lon', 'lat', 'h_elv', \\\n",
    "        's_elv','t_year', 'dh_dx', 'dh_dy', 'azimuth', 'window_width'])\n",
    "        \n",
    "        #convert from lat/lon to UTM zone 6\n",
    "        lon_1p, lat_1p = transformer.transform(lat1, lon1)\n",
    "        lon_2p, lat_2p = transformer.transform(lat2, lon2)\n",
    "        \n",
    "        for j in range(0, len(lon1)): #loop through points in first track \n",
    "            lon_a = lon_1p[j]\n",
    "            lat_a = lat_1p[j]\n",
    "            #get ditance between point a and  all points on second track           \n",
    "            distance = np.sqrt((lon_2p-lon_1p[j])**2 + (lat_2p-lat_1p[j])**2)\n",
    "            \n",
    "            #find closest point on second track\n",
    "            index = distance == min(distance)\n",
    "            lon_b = lon_2p[index]\n",
    "            lat_b = lat_2p[index]\n",
    "            \n",
    "            if(np.sqrt((lat_a-lat_b)**2 + (lon_a-lon_b)**2) > 5): #keep as a valid repeat point if points are within < 5m of each other\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            #get along and across-track distance between the points by projecting lat/lon (in UTM) to along/across track \n",
    "            #coordinates using the azimuth\n",
    "            az = az1[j] #azimuth of one track (we'll assume the two tracks rough approximately equal azimuth)\n",
    "            delta_Nc = lat_b - lat_a\n",
    "            delta_Ec = lon_b - lon_a\n",
    "            delta_x_c = delta_Nc*np.cos(np.radians(az)) + delta_Ec*np.sin(np.radians(az))\n",
    "            delta_y_c = delta_Nc*np.sin(np.radians(az)) - delta_Ec*np.cos(np.radians(az))\n",
    "            \n",
    "\n",
    "            \n",
    "            #pull out needed variables for both points\n",
    "            #segment elevation\n",
    "            h_1 = h1[j] \n",
    "            h_2 = h2[index][0]\n",
    "            #segment height uncertainty\n",
    "            sigma_h1 = sh_1[j] \n",
    "            sigma_h2 = sh_2[index][0]\n",
    "            #time\n",
    "            t_1 = t1[j]\n",
    "            t_2 = t2[index][0]\n",
    "            #along-track slope\n",
    "            dh_dx_1 = dh_dx1[j]\n",
    "            dh_dx_2 = dh_dx2[index][0]\n",
    "            #across-track slope\n",
    "            dh_dy_1 = dh_dy1[j]\n",
    "            dh_dy_2 = dh_dy2[index][0]\n",
    "            #fitting window\n",
    "            w_1 = window_1[j]\n",
    "            w_2 = window_2[index][0]\n",
    "            \n",
    "            \n",
    "            #filter by along-track slope slope:\n",
    "            if(abs((dh_dx_1)) > .05 or abs((dh_dx_2)) > .05):\n",
    "                continue\n",
    "            \n",
    "            #calculate elevation change\n",
    "            delta_h = h_2 - h_1\n",
    "            #propogate uncertainty\n",
    "            dh_sigma = np.sqrt(sigma_h1**2 + sigma_h2**2)\n",
    "\n",
    "            #save all variables\n",
    "            source_1.append(file_1)\n",
    "            lat_1.append(lat1[j])\n",
    "            lon_1.append(lon1[j])\n",
    "            height_1.append(h_1)\n",
    "            s_h1.append(sigma_h1)\n",
    "            slope_along_1.append(dh_dx_1)\n",
    "            slope_across_1.append(dh_dy_1)\n",
    "            dh.append(delta_h)\n",
    "            time_1.append(t_1)\n",
    "            distance_x.append(delta_x_c[0])\n",
    "            distance_y.append(delta_y_c[0])            \n",
    "            source_2.append(file_2)           \n",
    "            lon_2.append(lon2[index][0])\n",
    "            lat_2.append(lat2[index][0])\n",
    "            height_2.append(h_2)\n",
    "            s_h2.append(sigma_h2)\n",
    "            slope_along_2.append(dh_dx_2)\n",
    "            slope_across_2.append(dh_dy_2)\n",
    "            time_2.append(t_2)           \n",
    "            sigma_dh.append(dh_sigma)\n",
    "            dt.append(t_2 - t_1)\n",
    "            window1.append(w_1)\n",
    "            window2.append(w_2)\n",
    "        \n",
    "        if (i%10 == 0):  #progress check\n",
    "            print('pairs_checked:', i)\n",
    "    \n",
    "    #compile varaibles into a dictionary, then a dataframe\n",
    "    data_dict = {'source_1': source_1, 'source_2': source_2, 'lon_1': lon_1, 'lat_1': lat_1, 'lon_2': lon_2, 'lat_2': lat_2,\\\n",
    "                'h_1': height_1, 'sigma_h1': s_h1, 'h_2': height_2, 'sigma_h2': s_h2, 'dh': dh, \\\n",
    "                  'sigma_dh': sigma_dh, 'time_1': time_1, 'time_2': time_2, 'dt': dt,\n",
    "                'dh_dx_1': slope_along_1, 'dh_dx_2': slope_along_2, 'dh_dy_1': slope_across_1, 'dh_dy_2': slope_across_2, \\\n",
    "                 'distance_x': distance_x, 'window_1': window1, 'window_2': window2, 'distance_y': distance_y}\n",
    "    data = pd.DataFrame(data_dict)\n",
    "     #remove infinty values (which can occur for across-track slope)\n",
    "    data.replace([np.inf, -np.inf], np.nan, inplace = True) \n",
    "    data.dropna(inplace = True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    #add rgt and spot information into data frame\n",
    "    rgts = []\n",
    "    spot_n = [] #integer code for spot number (note: not the same as the beam number)\n",
    "    for i in range(0, len(data)):\n",
    "        rgts.append(get_rgt(data['source_1'][i]))\n",
    "        spot = get_spot(data['source_1'][i])\n",
    "        if spot == 'gt1l':\n",
    "            spot_n.append(1)\n",
    "        elif spot == 'gt1r':\n",
    "            spot_n.append(2)\n",
    "        elif spot == 'gt2l':\n",
    "            spot_n.append(3)\n",
    "        elif spot == 'gt2r':\n",
    "            spot_n.append(4)\n",
    "        elif spot == 'gt3l':\n",
    "            spot_n.append(5)\n",
    "        elif spot == 'gt3r':\n",
    "            spot_n.append(6)\n",
    "        else:\n",
    "            print('invalid beam')\n",
    "            spot_n.append(float('nan'))\n",
    "\n",
    "    data['rgt'] = rgts\n",
    "    data['spot_n'] = spot_n\n",
    "    end = time.time() #record total time elapsed\n",
    "    print('elapsed time: ', end - start)\n",
    "    os.chdir(\"..\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs_checked: 0\n",
      "pairs_checked: 10\n",
      "pairs_checked: 20\n",
      "pairs_checked: 30\n",
      "pairs_checked: 40\n",
      "pairs_checked: 50\n",
      "elapsed time:  57.371814489364624\n"
     ]
    }
   ],
   "source": [
    "#estimate repeat elevation elevation change from potenetial 2019 summer tracks\n",
    "repeat_dh = repeat_diffv2(repeats_summer_19, 'Anaktuvuk_v2_ATL06_reduced_all_slopes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output\n",
    "repeat_dh.to_csv('repeat_dh_filtered_slope_05_final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
