{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "This notebook goes the filtering and estimation of crossover statistic reported in Michaelides et al (2021) for various time intervals. Specifically, we:\n",
    "- Filter to remove outliers\n",
    "- Temporally subset the data into different 'thaw season' criteria for 2019 and 2020, based on data collection dates, temperature, and snow data\n",
    "- compile statistics for each time window and the full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define some functions to \n",
    "def get_rgt_cycle(fname):\n",
    "    return int(fname[31:39])\n",
    "def time_from_name(fname):\n",
    "    return int(fname[16:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll filter to remove outliers, defined as anyhting outside of two standard deviations from the mean. For the full crossover dataset, we'll also remove some unneeded columns to sreamline things for for further processing in matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xovers = pd.read_csv('xovers_full_period_low_slopes_new.csv')\n",
    "#filter out values outside of 2 standard deviations from the mean\n",
    "xovers_f = xovers[(xovers['dh'] < np.mean(xovers['dh']) + 2*np.std(xovers['dh'])) & (xovers['dh'] > np.mean(xovers['dh']) - 2*np.std(xovers['dh']))]\n",
    "#compare length before/after filtering\n",
    "print(len(xovers))\n",
    "print(len(xovers_f))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unneeded columns\n",
    "xovers_f.drop(columns = ['x', 'y','source_1', 'source_2', 'n_points(track_1)', 'n_points(track_2)', 'dh_dy_1', 'dh_dy_2',\\\n",
    "                         'track_1_fit', 'track_2_fit'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xovers_f.to_csv('xovers_full_period_low_slopes_new_filtered_demo.csv', index=False) #save filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter other crossover datasets by removing everything greater than 2 standard deviations from the mean\n",
    "xovers_40 = pd.read_csv('xovers_40_full_period_low_slopes_new.csv')\n",
    "xovers_40_addt = xovers_40[((xovers_40['time_1'] -2019)*365 >= 120) & ((xovers_40['time_2'] -2019)*365 >= 120) & \\\n",
    "                     ((xovers_40['time_1'] -2019*365) <= 290) & ((xovers_40['time_2'] -2019)*365 <= 290)]\n",
    "xovers_40_f = xovers_40[(xovers_40['dh'] <= np.mean(xovers_40['dh']) + 2*np.std(xovers_40['dh']))\\\n",
    "                                 & (xovers_40['dh'] >= np.mean(xovers_40['dh']) - 2*np.std(xovers_40['dh']))]\n",
    "\n",
    "xovers_60 = pd.read_csv('xovers_60_full_period_low_slopes_new.csv')\n",
    "xovers_60_addt = xovers_60[((xovers_60['time_1'] -2019)*365 >= 120) & ((xovers_60['time_2'] -2019)*365 >= 120) & \\\n",
    "                     ((xovers_60['time_1'] -2019*365) <= 290) & ((xovers_60['time_2'] -2019)*365 <= 290)]\n",
    "xovers_60_f = xovers_60[(xovers_60['dh'] <= np.mean(xovers_60['dh']) + 2*np.std(xovers_60['dh']))\\\n",
    "                                 & (xovers_60['dh'] >= np.mean(xovers_60['dh']) - 2*np.std(xovers_60['dh']))]\n",
    "\n",
    "\n",
    "xovers_80 = pd.read_csv('xovers_80_full_period_low_slopes_new.csv')\n",
    "xovers_80_addt = xovers_80[((xovers_80['time_1'] -2019)*365 >= 120) & ((xovers_80['time_2'] -2019)*365 >= 120) & \\\n",
    "                     ((xovers_80['time_1'] -2019*365) <= 290) & ((xovers_80['time_2'] -2019)*365 <= 290)]\n",
    "xovers_80_f = xovers_80[(xovers_80['dh'] <= np.mean(xovers_80['dh']) + 2*np.std(xovers_80['dh']))\\\n",
    "                                 & (xovers_80['dh'] >= np.mean(xovers_80['dh']) - 2*np.std(xovers_80['dh']))]\n",
    "\n",
    "xovers_100 = pd.read_csv('xovers_100_full_period_low_slopes_new.csv')\n",
    "xovers_100_addt = xovers_100[((xovers_100['time_1'] -2019)*365 >= 120) & ((xovers_100['time_2'] -2019)*365 >= 120) & \\\n",
    "                     ((xovers_100['time_1'] -2019*365) <= 290) & ((xovers_100['time_2'] -2019)*365 <= 290)]\n",
    "xovers_100_f = xovers_100[(xovers_100['dh'] <= np.mean(xovers_100['dh']) + 2*np.std(xovers_100['dh']))\\\n",
    "                                 & (xovers_100['dh'] >= np.mean(xovers_100['dh']) - 2*np.std(xovers_100['dh']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Subsetting and Statistics \n",
    "We now perform additional temporal subsetting based on different thresholds for the thaw season, to calculate the values reported in Table 1 of Michaelides et al (2021)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#InSAR window: March 8th to October 22nd\n",
    "xovers_insar_2019 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 67) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 67) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 295) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 295) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2019) & (np.floor(xovers_f['time_2']) == 2019)]\n",
    "#Period of positive Accumulated Degree Days of Thaw: April 30th to October 17th\n",
    "xovers_addt_2019 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 120) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 120) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 290) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 290) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2019) & (np.floor(xovers_f['time_2']) == 2019)]\n",
    "#Snow-free interval based on MODIS imagery: June 6th to September 19th\n",
    "xovers_visual_2019 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 157) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 157) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 260) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 260) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2019) & (np.floor(xovers_f['time_2']) == 2019)]\n",
    "#Snow-free interval based on MERRA-2: June 1st to August 31st\n",
    "xovers_merra_2019 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 152) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 152) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 243) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 243) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2019) & (np.floor(xovers_f['time_2']) == 2019)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repeat for 2020\n",
    "#InSAR window: March 8th to October 22nd\n",
    "xovers_insar_2020 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 68) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 68) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 296) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 296) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2020) & (np.floor(xovers_f['time_2']) == 2020)]\n",
    "#Period of positive Accumulated Degree Days of Thaw: April 30th to October 17th\n",
    "xovers_addt_2020 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 121) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 121) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 291) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 291) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2020) & (np.floor(xovers_f['time_2']) == 2020)]\n",
    "#Snow-free interval based on MODIS imagery: June 6th to September 19th\n",
    "xovers_visual_2020 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 163) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 163) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 262) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 262) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2020) & (np.floor(xovers_f['time_2']) == 2020)]\n",
    "#Snow-free interval based on MERRA-2: June 1st to August 31st\n",
    "xovers_merra_2020 = xovers_f[((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 >= 153) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 >= 153) & \\\n",
    "                     ((xovers_f['time_1'] -np.floor(xovers_f['time_1']))*365 <= 244) & ((xovers_f['time_2'] -np.floor(xovers_f['time_2']))*365 <= 244) & \\\n",
    "                      (np.floor(xovers_f['time_1']) == 2020) & (np.floor(xovers_f['time_2']) == 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate statistics for different time intervals\n",
    "\n",
    "#define date windows\n",
    "names = ['original', 'DAYMET thaw/refreeze period', 'Snow free imagery', \\\n",
    "         'MERRA low snow']\n",
    "date_1 = ['3/8/2019', '4/30/2019', '6/06/2019', '6/01/2019']\n",
    "date_2 = ['10/22/2019', '10/17/2019', '9/17/2019', '8/31/2019']\n",
    "\n",
    "medians_2019 = [np.median(xovers_insar_2019['dh']*100), np.median(xovers_addt_2019['dh']*100), np.median(xovers_visual_2019['dh']*100),\\\n",
    "           np.median(xovers_merra_2019['dh']*100)] #median dh (cm) for each window\n",
    "means_2019 = [np.mean(xovers_insar_2019['dh']*100), np.mean(xovers_addt_2019['dh']*100), np.mean(xovers_visual_2019['dh']*100), \\\n",
    "         np.mean(xovers_merra_2019['dh']*100)] #mean dh (cm) for each window\n",
    "mins_2019 = [np.min(xovers_insar_2019['dh']*100), np.min(xovers_addt_2019['dh']*100), np.min(xovers_visual_2019['dh']*100), \\\n",
    "             np.min(xovers_merra_2019['dh']*100)]#minimum dh (cm) for each window\n",
    "maxes_2019 = [np.max(xovers_insar_2019['dh']*100), np.max(xovers_addt_2019['dh']*100), np.max(xovers_visual_2019['dh']*100), \\\n",
    "              np.max(xovers_merra_2019['dh']*100)]#maximum dh (cm) for each window\n",
    "sigmas_2019 = [np.std(xovers_insar_2019['dh']*100), np.std(xovers_addt_2019['dh']*100), np.std(xovers_visual_2019['dh']*100), \\\n",
    "               np.std(xovers_merra_2019['dh']*100)] #standard deviation of dh (cm) for each window\n",
    "n_total_2019 = [len(xovers_insar_2019), len(xovers_addt_2019), len(xovers_visual_2019), len(xovers_merra_2019)] #number of crossovers in each windwo\n",
    "n_short_2019 = [len(xovers_insar_2019[xovers_insar_2019['dt (days)'] <= 14]), \\\n",
    "                len(xovers_addt_2019[xovers_addt_2019['dt (days)'] <= 14]), \\\n",
    "           len(xovers_visual_2019[xovers_visual_2019['dt (days)'] <= 14]), \\\n",
    "                len(xovers_merra_2019[xovers_merra_2019['dt (days)'] <= 14]) ] #number of short-period (< 14 days) crossovers in each interval\n",
    "sigma_short_2019 = [np.std(xovers_insar_2019[xovers_insar_2019['dt (days)'] <= 14]['dh']*100), \\\n",
    "                    np.std(xovers_addt_2019[xovers_addt_2019['dt (days)'] <= 14]['dh']*100), \\\n",
    "              np.std(xovers_visual_2019[xovers_visual_2019['dt (days)'] <= 14]['dh']*100),\\\n",
    "                    np.std(xovers_merra_2019[xovers_merra_2019['dt (days)'] <= 14]['dh']*100)] #standard deviation of short-period crossovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in normalized accumulated degree days of thaw data, which account for both thaw (increasing ADDT) and refreeze (decreasing ADDT)\n",
    "temp = pd.read_csv('NADDT_curve.csv')\n",
    "doy = temp['DOY'].values\n",
    "naddt = temp['NADDT'].values\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of crossovers that correspond to positive and negative changes in ADDT for each window for 2019\n",
    "n_thaw_2019 = []\n",
    "n_freeze_2019 = []\n",
    "insar_thaw_count_19 = 0\n",
    "insar_freeze_count_19 = 0\n",
    "\n",
    "for i in range(0, len(xovers_insar_2019)):\n",
    "    #get DOY for each xover\n",
    "    doy_1 = int(np.floor((xovers_insar_2019['time_1'].values[i] - 2019)*365))\n",
    "    doy_2 = int(np.floor((xovers_insar_2019['time_2'].values[i] - 2019)*365))\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs DOY starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        insar_thaw_count_19 = insar_thaw_count_19+1\n",
    "    elif delta_addt < 0:\n",
    "        insar_freeze_count_19 = insar_freeze_count_19 + 1\n",
    "#add counts to list    \n",
    "n_thaw_2019.append(insar_thaw_count_19)\n",
    "n_freeze_2019.append(insar_freeze_count_19)\n",
    "\n",
    "#repeat for other windows\n",
    "addt_thaw_count_19 = 0\n",
    "addt_freeze_count_19 = 0\n",
    "for i in range(0, len(xovers_addt_2019)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_addt_2019['time_1'].values[i] - 2019)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_addt_2019['time_2'].values[i] - 2019)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        addt_thaw_count_19 = addt_thaw_count_19+1\n",
    "    elif delta_addt < 0:\n",
    "        addt_freeze_count_19 = addt_freeze_count_19 + 1\n",
    "\n",
    "n_thaw_2019.append(addt_thaw_count_19)\n",
    "n_freeze_2019.append(addt_freeze_count_19)\n",
    "\n",
    "visual_thaw_count_19 = 0\n",
    "visual_freeze_count_19 = 0\n",
    "for i in range(0, len(xovers_visual_2019)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_visual_2019['time_1'].values[i] - 2019)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_visual_2019['time_2'].values[i] - 2019)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        visual_thaw_count_19 = visual_thaw_count_19+1\n",
    "    elif delta_addt < 0:\n",
    "        visual_freeze_count_19 = visual_freeze_count_19 + 1\n",
    "\n",
    "n_thaw_2019.append(visual_thaw_count_19)\n",
    "n_freeze_2019.append(visual_freeze_count_19)\n",
    "\n",
    "merra_thaw_count_19 = 0\n",
    "merra_freeze_count_19 = 0\n",
    "for i in range(0, len(xovers_merra_2019)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_merra_2019['time_1'].values[i] - 2019)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_merra_2019['time_2'].values[i] - 2019)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        merra_thaw_count_19 = merra_thaw_count_19+1\n",
    "    elif delta_addt < 0:\n",
    "        merra_freeze_count_19 = merra_freeze_count_19 + 1\n",
    "\n",
    "n_thaw_2019.append(merra_thaw_count_19)\n",
    "n_freeze_2019.append(merra_freeze_count_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put all the data into a dictionary\n",
    "data_dict_2019 = {'window': names, 'start date': date_1, 'end date': date_2, 'total crossovers':n_total_2019, 'n_thaw':n_thaw_2019, 'n_freeze':n_freeze_2019, 'mean dh (cm)': means_2019, \\\n",
    "            'median dh (cm)':medians_2019, 'min dh (cm)':mins_2019, 'max dh (cm)':maxes_2019, 'standard deviation (cm)':sigmas_2019,\\\n",
    "             'short period crossovers':n_short_2019, 'short-period standard deviation (cm)':sigma_short_2019 }\n",
    "data_table_2019 = pd.DataFrame(data_dict_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a look at the statistics for 2019\n",
    "display(data_table_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agregate statistics for 2020\n",
    "names = ['original', 'DAYMET thaw/refreeze period', 'Snow free imagery', \\\n",
    "         'MERRA low snow']\n",
    "date_1 = ['3/8/2020', '4/30/2020', '6/11/2020', '6/01/2020']\n",
    "date_2 = ['10/22/2020', '10/17/2020', '9/18/2020', '8/31/2020']\n",
    "medians_2020 = [np.median(xovers_insar_2020['dh']*100), np.median(xovers_addt_2020['dh']*100), np.median(xovers_visual_2020['dh']*100),\\\n",
    "           np.median(xovers_merra_2020['dh']*100)] #median dh (cm) for each window\n",
    "means_2020 = [np.mean(xovers_insar_2020['dh']*100), np.mean(xovers_addt_2020['dh']*100), np.mean(xovers_visual_2020['dh']*100), \\\n",
    "         np.mean(xovers_merra_2020['dh']*100)] #mean dh (cm) for each window\n",
    "mins_2020 = [np.min(xovers_insar_2020['dh']*100), np.min(xovers_addt_2020['dh']*100), np.min(xovers_visual_2020['dh']*100), \\\n",
    "             np.min(xovers_merra_2020['dh']*100)] #minimum dh (cm) for each window\n",
    "maxes_2020 = [np.max(xovers_insar_2020['dh']*100), np.max(xovers_addt_2020['dh']*100), np.max(xovers_visual_2020['dh']*100), \\\n",
    "              np.max(xovers_merra_2020['dh']*100)] #maximum dh (cm) for each window\n",
    "sigmas_2020 = [np.std(xovers_insar_2020['dh']*100), np.std(xovers_addt_2020['dh']*100), np.std(xovers_visual_2020['dh']*100),\\\n",
    "               np.std(xovers_merra_2020['dh']*100)] #standard deviation of dh (cm) for each window\n",
    "n_total_2020 = [len(xovers_insar_2020), len(xovers_addt_2020), len(xovers_visual_2020), len(xovers_merra_2020)] #number of crossovers in each window\n",
    "n_short_2020 = [len(xovers_insar_2020[xovers_insar_2020['dt (days)'] <= 14]), len(xovers_addt_2020[xovers_addt_2020['dt (days)'] <= 14]), \\\n",
    "           len(xovers_visual_2020[xovers_visual_2020['dt (days)'] <= 14]), len(xovers_merra_2020[xovers_merra_2020['dt (days)'] <= 14]) ] #number of short-period (< 14 days) crossovers in each interval\n",
    "\n",
    "sigma_short_2020 = [np.std(xovers_insar_2020[xovers_insar_2020['dt (days)'] <= 14]['dh']*100), \\\n",
    "                    np.std(xovers_addt_2020[xovers_addt_2020['dt (days)'] <= 14]['dh']*100), \\\n",
    "              np.std(xovers_visual_2020[xovers_visual_2020['dt (days)'] <= 14]['dh']*100), \\\n",
    "                    np.std(xovers_merra_2020[xovers_merra_2020['dt (days)'] <= 14]['dh']*100)] #standard deviation of short-period crossovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of crossovers that correspond to positive and negative changes in ADDT for each window\n",
    "\n",
    "n_thaw_2020 = []\n",
    "n_freeze_2020 = []\n",
    "insar_thaw_count_20 = 0\n",
    "insar_freeze_count_20 = 0\n",
    "\n",
    "for i in range(0, len(xovers_insar_2020)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_insar_2020['time_1'].values[i] - 2020)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_insar_2020['time_2'].values[i] - 2020)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        insar_thaw_count_20 = insar_thaw_count_20+1\n",
    "    elif delta_addt < 0:\n",
    "        insar_freeze_count_20 = insar_freeze_count_20 + 1\n",
    "    \n",
    "n_thaw_2020.append(insar_thaw_count_20)\n",
    "n_freeze_2020.append(insar_freeze_count_20)\n",
    "\n",
    "addt_thaw_count_20 = 0\n",
    "addt_freeze_count_20 = 0\n",
    "for i in range(0, len(xovers_addt_2020)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_addt_2020['time_1'].values[i] - 2020)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_addt_2020['time_2'].values[i] - 2020)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        addt_thaw_count_20 = addt_thaw_count_20+1\n",
    "    elif delta_addt < 0:\n",
    "        addt_freeze_count_20 = addt_freeze_count_20 + 1\n",
    "\n",
    "n_thaw_2020.append(addt_thaw_count_20)\n",
    "n_freeze_2020.append(addt_freeze_count_20)\n",
    "\n",
    "visual_thaw_count_20 = 0\n",
    "visual_freeze_count_20 = 0\n",
    "for i in range(0, len(xovers_visual_2020)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_visual_2020['time_1'].values[i] - 2020)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_visual_2020['time_2'].values[i] - 2020)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        visual_thaw_count_20 = visual_thaw_count_20+1\n",
    "    elif delta_addt < 0:\n",
    "        visual_freeze_count_20 = visual_freeze_count_20 + 1\n",
    "\n",
    "n_thaw_2020.append(visual_thaw_count_20)\n",
    "n_freeze_2020.append(visual_freeze_count_20)\n",
    "\n",
    "merra_thaw_count_20 = 0\n",
    "merra_freeze_count_20 = 0\n",
    "for i in range(0, len(xovers_merra_2020)):\n",
    "    #get doy for each xover\n",
    "    doy_1 = int(np.floor((xovers_merra_2020['time_1'].values[i] - 2020)*365))\n",
    "    #print(doy_1)\n",
    "    doy_2 = int(np.floor((xovers_merra_2020['time_2'].values[i] - 2020)*365))\n",
    "    #print(doy_2)\n",
    "    delta_addt = naddt[doy_2 - 1] - naddt[doy_1 - 1] #get change in naddt (-1 accounts for the 0 indexing vs doy starting at 1)\n",
    "    #check if xover corresponds to freeze or thaw (or neither)\n",
    "    if delta_addt > 0:\n",
    "        merra_thaw_count_20 = merra_thaw_count_20+1\n",
    "    elif delta_addt < 0:\n",
    "        merra_freeze_count_20 = merra_freeze_count_20 + 1\n",
    "\n",
    "n_thaw_2020.append(merra_thaw_count_20)\n",
    "n_freeze_2020.append(merra_freeze_count_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile 2020 data into dataframe\n",
    "data_dict_2020 = {'window': names, 'start date': date_1, 'end date': date_2, 'total crossovers':n_total_2020,'n_thaw':n_thaw_2020, 'n_freeze':n_freeze_2020, 'mean dh (cm)': means_2020, \\\n",
    "            'median dh (cm)':medians_2020, 'min dh (cm)':mins_2020, 'max dh (cm)':maxes_2020, 'standard deviation (cm)':sigmas_2020,\\\n",
    "             'short period crossovers':n_short_2020, 'short-period standard deviation (cm)':sigma_short_2020}\n",
    "data_table_2020 = pd.DataFrame(data_dict_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data_table_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save 2019 and 2020 metrics\n",
    "data_table_2019.to_csv('xover_time_windows_stats_2019_demo.csv', index = False)\n",
    "data_table_2020.to_csv('xover_time_windows_stats_2020_demo.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscellaneous statistics\n",
    "This section goes through the calcualtion of the general statistics reported in the crossovers section of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tally total number of files\n",
    "os.chdir('Anaktuvuk_v2_ATL06_reduced_all_slopes')\n",
    "files = glob.glob('*.h5')\n",
    "track_list = []\n",
    "for f in files:\n",
    "    track_list.append(get_rgt_cycle(f)) #record rgt and cycle\n",
    "print(len(set(track_list))) #get unique tracks \n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xovers_40_f.to_csv('xovers_40_full_period_low_slopes_filtered.csv')\n",
    "xovers_60_f.to_csv('xovers_60_full_period_low_slopes_filtered.csv')\n",
    "xovers_80_f.to_csv('xovers_80_full_period_low_slopes_filtered.csv')\n",
    "xovers_100_f.to_csv('xovers_100_full_period_low_slopes_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of crossovers post-filtering for different radii\n",
    "print(len(xovers_f))\n",
    "print(len(xovers_40_f))\n",
    "print(len(xovers_60_f))\n",
    "print(len(xovers_80_f))\n",
    "print(len(xovers_100_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get number of short-period crossovers post-filtering for different radii\n",
    "print(len(xovers_f[xovers_f['dt (days)'] <= 14]))\n",
    "print(len(xovers_40_f[xovers_40_f['dt (days)'] <= 14]))\n",
    "print(len(xovers_60_f[xovers_60_f['dt (days)'] <= 14]))\n",
    "print(len(xovers_80_f[xovers_80_f['dt (days)'] <= 14]))\n",
    "print(len(xovers_100_f[xovers_100_f['dt (days)'] <= 14]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check minimum time interval for each file\n",
    "print(np.min(xovers_f[xovers_f['dt (days)'] > 1]['dt (days)'].values))\n",
    "print(np.min(xovers_40_f[xovers_40_f['dt (days)'] > 1]['dt (days)'].values))\n",
    "print(np.min(xovers_60_f[xovers_60_f['dt (days)'] > 1]['dt (days)'].values))\n",
    "print(np.min(xovers_80_f[xovers_80_f['dt (days)'] > 1]['dt (days)'].values))\n",
    "print(np.min(xovers_100_f[xovers_100_f['dt (days)'] > 1]['dt (days)'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check maximum time interval for each file\n",
    "print(np.max(xovers_f['dt (days)']))\n",
    "print(np.max(xovers_40_f['dt (days)']))\n",
    "print(np.max(xovers_60_f['dt (days)']))\n",
    "print(np.max(xovers_80_f['dt (days)']))\n",
    "print(np.max(xovers_100_f['dt (days)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
